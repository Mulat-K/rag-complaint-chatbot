{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e19e22",
   "metadata": {},
   "source": [
    "# Cell 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39300d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f057fea",
   "metadata": {},
   "source": [
    "# 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a93ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/filtered_complaints.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902736f",
   "metadata": {},
   "source": [
    "# 2. Stratified Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab0701",
   "metadata": {},
   "source": [
    "## We take a sample of 10,000 for efficiency in this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6248e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 10000\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    # Stratify by Product Category to ensure representation\n",
    "    df_sample, _ = train_test_split(\n",
    "        df, \n",
    "        train_size=SAMPLE_SIZE, \n",
    "        stratify=df['Product_Category'], \n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    df_sample = df\n",
    "\n",
    "print(f\"Sample size: {len(df_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fa94b",
   "metadata": {},
   "source": [
    "# 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384893b",
   "metadata": {},
   "source": [
    "## We need to split long text into smaller chunks for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50175ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents...\n",
      "Total chunks created: 28837\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "print(\"Chunking documents...\")\n",
    "for idx, row in df_sample.iterrows():\n",
    "    narrative = row['cleaned_narrative']\n",
    "    if pd.isna(narrative) or narrative == \"\":\n",
    "        continue\n",
    "        \n",
    "    chunks = text_splitter.split_text(narrative)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(chunk)\n",
    "        metadatas.append({\n",
    "            \"complaint_id\": str(row.get('Complaint ID', idx)), # Use ID if available, else index\n",
    "            \"product_category\": row['Product_Category'],\n",
    "            \"issue\": row.get('Issue', 'Unknown'),\n",
    "            \"chunk_index\": i\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01d496",
   "metadata": {},
   "source": [
    "# 4. Embedding & Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d3c20",
   "metadata": {},
   "source": [
    "## Define the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2456292d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13988e5c2a754c3886c238d115f240b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mulat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mulat\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bfc0dbb28a41dcb972678291f69f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e611fbec8422284bb1331d09e2317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbfefbaa78341b9b91c02891458c0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3969aa3041e42c3aa5aea0465e7caad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364bf7b935494d73ae76a5a42ea16ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3cc43d8c504ba2ad1f3a2dabfa5f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b25d93e4d6436a95d82c632b2d5a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805117bfdc094033b58fe454a3913e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c1610fd4b540b6a9b1c54521083ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa800e00c8f45279049d5b34875d9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6bc7c",
   "metadata": {},
   "source": [
    "## Define vector store path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3302e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../vector_store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75729db5",
   "metadata": {},
   "source": [
    "## Clear existing vector store if it exists (to avoid duplicates during testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90f8bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector Store (this may take a few minutes)...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "print(\"Creating Vector Store (this may take a few minutes)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961fdc9",
   "metadata": {},
   "source": [
    "## Create ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5fcaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and saved to ../vector_store\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma.from_texts(\n",
    "    texts=documents,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"Vector store created and saved to {persist_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
